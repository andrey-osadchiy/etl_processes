# üìå–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–±–æ—Ç–∞ (—ç–∫–∑–∞–º–µ–Ω)
## üîπ–ó–∞–¥–∞–Ω–∏–µ 1: –†–∞–±–æ—Ç–∞ —Å Yandex DataTransfer
### –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ Managed Service for YDB –≤ –æ–±—ä–µ–∫—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ Object Storage. –í—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ—Ä–≤–∏—Å–∞ Data Transfer.
1) –°–æ–∑–¥–∞—ë–º –±–∞–∫–µ—Ç 
 ![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/1.png)
2) –°–æ–∑–¥–∞—ë–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö YDB
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2.png)
3) –í YDB —Å–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É (–≤ –º–æ—ë–º —Å–ª—É—á–∞–µ —Ç–∞–±–ª–∏—Ü–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º –º–æ–µ–π —Å–µ–º—å–∏)
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/3.png)
4) –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/4.png)
5) –ü–æ–ª—É—á–∏–ª–∞—Å—å —Ç–∞–∫–∞—è —Ç–∞–±–ª–∏—Ü–∞
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/5.png)
6)  –°–æ–∑–¥–∞—ë–º —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã (–ø—Ä–∏—ë–º–Ω–∏–∫ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫) –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/6.png)
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/06.png)
7)  –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫ –ø—Ä–æ—à–µ–ª —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä, –∑–∞—Ö–æ–¥–∏–º –≤ —Ä–∞–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –Ω–∞–º–∏ –±–∞–∫–µ—Ç
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/7.png)
8)  –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª, —Å–º–æ—Ç—Ä–∏–º –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ.
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/8.png)
–í—Å—ë –∫–∞–∫ –Ω–∞–¥–æ. –ö–æ–Ω–µ—á–Ω—ã–π —Ñ–∞–π–ª –ª–µ–∂–∏—Ç [—Ç—É—Ç](https://github.com/andrey-osadchiy/etl_processes/tree/main/final_project/staff)

## üîπ–ó–∞–¥–∞–Ω–∏–µ 2: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å Yandex Data Processing –ø—Ä–∏ –ø–æ–º–æ—â–∏ Apache AirFlow
### –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª—ã (parquet –∏–ª–∏ CSV) –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞. –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –º–µ–Ω—è–µ—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –¥–Ω–∏ –º–µ—Å—è—Ü–∞.  
–°–Ω–∞—áa–ª–∞ –≤ —Ä–∞–Ω–µ–µ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –±–∞–∫–µ—Ç–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏. –í –ø–∞–ø–∫–µ dags –±—É–¥–µ—Ç –ª–µ–∂–∞—Ç—å –¥–∞–≥, –≤ scripts –Ω–∞—à —Å–∫—Ä–∏–ø—Ç. –í –ø–∞–ø–∫–µ —Ñ–∞–π–ª –Ω–∞—à —Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–µ–º—å–∏ –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è. –í results –º—ã –∑–∞—Å—É–Ω–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª.
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/1.png)
–î–∞–ª–µ–µ –º—ã —Å–æ–∑–¥–∞—ë–º –∫–ª–∞—Å—Ç–µ—Ä—ã. –ö–ª–∞—Å—Ç–µ—Ä AirFlow, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∞—Ä–∫–µ—Å—Ç—Ä–∞—Ä–æ—Ç–æ–º (–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫) –∏ –ö–ª–∞—Å—Ç–µ—Ä Data Proc (–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ PySpark-–∑–∞–¥–∞–Ω–∏–π)

–°–Ω–∞—á–ª–∞ Data Proc
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/2.png)
–ü–æ—Ç–æ–º AirFlow
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/22.png)
–î–∞–ª–µ–µ –≥–æ—Ç–æ–≤–∏–º —Ñ–∞–π–ª—ã data_processing_dag.py 
```python

from airflow import DAG
from airflow.providers.yandex.operators.yandexcloud_dataproc import DataprocCreatePysparkJobOperator
from datetime import datetime

YC_DP_CLUSTER_ID = 'c9quhbgni0nl9loms74k'
YC_BUCKET = 'examen'

with DAG(
    'family_data_processing_v3',
    schedule_interval='@daily',
    start_date=datetime(2025, 6, 1),
    catchup=False,
    tags=['data-processing']
) as dag:

    process_data = DataprocCreatePysparkJobOperator(
        task_id='process_data',
        cluster_id=YC_DP_CLUSTER_ID,
        main_python_file_uri=f's3a://{YC_BUCKET}/scripts/process_data.py',
        args=[
            '--input', f's3a://{YC_BUCKET}/file/family.csv',
            '--output', f's3a://{YC_BUCKET}/results/{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        ],
        properties={
            'spark.sql.legacy.timeParserPolicy': 'LEGACY',  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
            'spark.driver.memory': '2g'
        }
    )
```
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/4.png)

–∏ process_data.py
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, to_date
import sys

def main():
    spark = SparkSession.builder \
        .appName("FamilyDataProcessing") \
        .config("spark.hadoop.fs.s3a.endpoint", "storage.yandexcloud.net") \
        .getOrCreate()

    try:
        args = {k: v for k, v in zip(sys.argv[1::2], sys.argv[2::2])}
        input_path = args.get('--input', 's3://examen/file/family.csv')
        output_path = args.get('--output', 's3://examen/results/family_processed')
  
        df = spark.read.csv(input_path, header=False, schema="id INT, date STRING, name STRING, is_male BOOLEAN")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã (—Ñ–æ—Ä–º–∞—Ç: "1993-04-14 00:00:00 +0000 UTC")
        df = df.withColumn("date", to_date(col("date").substr(1, 10), "yyyy-MM-dd"))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        print("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
        df.show(5, truncate=False)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—è
        result_df = df.filter(year(col("date")) >= 2001) \
                    .groupBy("is_male") \
                    .count()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        result_df.write.mode("overwrite").parquet(output_path)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")
        return 0

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {str(e)}", file=sys.stderr)
        return 1
    finally:
        spark.stop()

if __name__ == "__main__":
    sys.exit(main())
```
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/5.png)
–§–∞–π–ª—ã –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [—Ç—É—Ç](https://github.com/andrey-osadchiy/etl_processes/tree/main/final_project/staff)

–î–∞–ª–µ–µ –ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ airflow –ø–æ —Ä–∞–Ω–µ–µ –Ω–∞–º–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Ä–µ–∫–≤–∏–∑–∏—Ç–∞–º
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/6.png)

–ó–∞–ø—É—Å–∫–∞–µ–º –Ω–∞—à –¥–∞–≥
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/7.png)
–í—Å—ë —É—Å–ø–µ—à–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/2/8.png)
–û—Ç–ª–∏—á–Ω–æ. –í—Å—ë –ø–æ–ª—É—á–∏–ª–æ—Å—å. –û—Å—Ç–∞–ª–æ—Å—å —Ç–æ–ª—å–∫–æ —É–¥–∞–ª–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/9.png)


## üîπ–ó–∞–¥–∞–Ω–∏–µ 4 (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ): –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ DataLens
### –° –ø–æ–º–æ—â—å—é Yandex DataLens –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. 
–°–æ–∑–¥–∞—ë–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ YDB –≥–¥–µ –ª–µ–∂–∏—Ç –Ω–∞—à–∞ —Ç–∞–±–ª–∏—Ü–∞
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/4/1.png)
–°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/4/3.png)
–ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–∑–¥–∞—ë–º —á–∞—Ä—Ç—ã,–≥—Ä–∞—Ñ–∏–∫–∏, –¥–∏–∞–≥—Ä–∞–º–º—ã
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/4/4.png)
–°–æ–∑–¥–∞—ë–º –¥–∞—à–±–æ—Ä–¥,  –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è –∏ –ø–æ–∏–≥—Ä–∞—Ç—å—Å—è —Å –Ω–∏–º –º–æ–∂–Ω–æ [—Ç—É—Ç](https://datalens.yandex.cloud/1mxcyr9l58xwm)
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/4/6.png)

## üîπ–ó–∞–¥–∞–Ω–∏–µ 3: –†–∞–±–æ—Ç–∞ —Å —Ç–æ–ø–∏–∫–∞–º–∏ Apache Kafka¬Æ —Å –ø–æ–º–æ—â—å—é PySpark-–∑–∞–¥–∞–Ω–∏–π –≤ Yandex Data Processing
### –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á—Ç–µ–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤ kafka –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
–õ–æ–≥–∏–∫–∞ —Ç–∞–∫–∞—è: 
1) –±–µ—Ä–µ–º —Ñ–∞–π–ª family.csv –∫–æ—Ç–æ—ã—Ä–π –ª–µ–∂–∏—Ç –≤ Object Storage
2) –ø—Ä–∏ –ø–æ–º–æ—â–∏ Kafka Producer —á–∏—Ç–∞–µ–º CSV –∏–∑ –±–∞–∫–µ—Ç–∞ (–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É –≤ JSON.)
3) –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka-—Ç–æ–ø–∏–∫ family-topic
4) Kafka Consumer —Å–ª—É—à–∞–µ—Ç —Ç–æ–ø–∏–∫ family-topic –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–∞—Ä—Å–∏—Ç JSON –∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ YDB —á–µ—Ä–µ–∑ UPSERT
5) –ø–æ –∏—Ç–æ–≥—É –¥–∞–Ω–Ω—ã–µ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ YDB family_members

–°–æ–∑–¥–∞—ë–º —Ç–∞–±–ª–∏—Ü—É –≤ YDB –∫—É–¥–ø –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è


–°–æ–∑–¥–∞—ë–º –±–∞–∫–µ—Ç —Å–æ –≤—Å–µ–º–∏ —Ñ–∞–π–ª–∞–º–∏. —Ç–æ—Ç –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è, –∞ —Ç–∞–∫–∂–µ 
kafka-write.py - –∑–∞–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV (Object Storage) –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ Kafka.
```python
import time
import random
import json
from kafka import KafkaProducer
import csv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kafka Producer
producer = KafkaProducer(
    bootstrap_servers='rc1a-sp0t812fps48sn74.mdb.yandexcloud.net:9091',
    security_protocol='SASL_SSL',
    sasl_mechanism='SCRAM-SHA-512',
    sasl_plain_username='user1',
    sasl_plain_password='password1',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV 
with open('gs://examen/kafka/family.csv', 'r') as file:
    reader = csv.reader(file)
    data = list(reader)  # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏

# –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Kafka (–ø–æ 1 –∑–∞–ø–∏—Å–∏ –≤ —Å–µ–∫—É–Ω–¥—É)
while True:
    row = random.choice(data)  # –ë–µ—Ä—ë–º —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–æ–∫—É
    id, birth_date, name_json, is_male = row
    message = {
        "id": id,
        "birth_date": birth_date,
        "first_name": json.loads(name_json)["first_name"],
        "is_male": is_male.lower() == "true"
    }
    producer.send('family-topic', value=message)
    print(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {message}")
    time.sleep(1)  # –ü–∞—É–∑–∞ 1 —Å–µ–∫—É–Ω–¥–∞
```
–∏ kafka-read-ydb.py ‚Äî —á–∏—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ YDB

```python
import json
from kafka import KafkaConsumer
import ydb
import os
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ YDB
driver_config = ydb.DriverConfig(
    endpoint="grpcs://ydb.serverless.yandexcloud.net:2135",
    database="/ru-central1/b1ga28ro2ctk606jpmoh/etnusig6loit8ui9gkmb",
    credentials=ydb.construct_credentials_from_environ(),  # –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ IAM
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Kafka
consumer = KafkaConsumer(
    'family-topic',
    bootstrap_servers='ydb-03.serverless.yandexcloud.net:9093',
    security_protocol='SASL_SSL',
    sasl_mechanism='SCRAM-SHA-512',
    sasl_plain_username='user1',  
    sasl_plain_password='password1',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ YDB
driver = ydb.Driver(driver_config)
driver.wait(fail_fast=True, timeout=5)
session = driver.table_client.session().create()

def upsert_family_member(data):
    query = f"""
    UPSERT INTO `family_members` (id, birth_date, first_name, is_male)
    VALUES (
        {data['id']},
        CAST('{data['birth_date']}' AS Timestamp),
        '{data['first_name']}',
        {data['is_male']}
    )
    """
    session.transaction().execute(query, commit_tx=True)

print("–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka...")
for message in consumer:
    data = message.value
    print(f"–ü–æ–ª—É—á–µ–Ω–æ: {data}")
    upsert_family_member(data)
```
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/3/3.png)

–ü–æ—Å–ª–µ–¥–Ω–∏–º —à–∞–≥–æ–º —Å–æ–∑–¥–∞—ë–º –∑–∞–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—Å—É–∫–∞–µ–º –µ–≥–æ
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/3/4.png)


–£–¥–∞–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/3/5.png)
![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/3/6.png)
